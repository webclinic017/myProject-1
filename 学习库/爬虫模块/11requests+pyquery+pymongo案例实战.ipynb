{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c1c346",
   "metadata": {},
   "source": [
    "我们以一个基本的静态网站作为案例进行爬取，需要爬取的链接为： https://static1.scrape.cuiqingcai.com/   \n",
    "本节的目标：  \n",
    "1.用 requests 爬取这个站点每一页的电影列表，顺着列表再爬取每个电影的详情页。  \n",
    "2.用 pyquery 和正则表达式提取每部电影的名称、封面、类别、上映时间、评分、剧情简介等。  \n",
    "3.把以上爬取的内容存入 MongoDB 数据库。  \n",
    "4.使用多进程实现爬取的加速。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52104d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #  用来爬取页面\n",
    "import logging #  用来输出信息\n",
    "import re # 用来实现正则表达式解析\n",
    "import pymongo  # 用来实现 MongoDB  存储\n",
    "from pyquery import PyQuery as pq # 用来直接解析网页\n",
    "from urllib.parse import urljoin # 用来做 URL 的拼接\n",
    "from requests.packages import urllib3\n",
    "urllib3.disable_warnings() # 忽略证书警告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405abcf",
   "metadata": {},
   "source": [
    "1. 定义日志输出级别和输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32b44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, # 输出级别\n",
    "                   format='%(asctime)s-%(levelname)s:%(message)s') # 输出格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a550f",
   "metadata": {},
   "source": [
    "2. 定义根URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9338572",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://static1.scrape.cuiqingcai.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33afb5fa",
   "metadata": {},
   "source": [
    "3. 爬取总页数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278fd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_PAGE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fcaff7",
   "metadata": {},
   "source": [
    "4. 定义函数：接收一个url参数，返回页面的html.text代码。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4504c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    logging.info(f'正在爬取{url}……')\n",
    "    try:\n",
    "        response = requests.get(url, verify=False)\n",
    "    except requests.RequestException:\n",
    "        logging.error('爬取%s时发生了一个错误', url, exc_info=True)\n",
    "        # 这时，将logging.error的exc_info参数设置为True可以打印出Traceback错误信息。\n",
    "    else:\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            logging.error(f'爬取{url}失败，错误代码{response.status_code}。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e89c8",
   "metadata": {},
   "source": [
    "5. 定义函数：接收页码参数, 返回该页的html.text代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832cfb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_index(p):\n",
    "    index_url = f'{BASE_URL}/page/{p}'\n",
    "    return scrape_page(index_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3a549",
   "metadata": {},
   "source": [
    "6. 定义函数：接收html.text代码, 返回该页每部电影网址（拼接后）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c93c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index(html):\n",
    "    doc = pq(html)\n",
    "    links = doc('.el-card .name')\n",
    "    for i in links.items():\n",
    "        href = i.attr('href') # 找到小网址\n",
    "        detail_url = urljoin(BASE_URL, href) # 拼接起来，扔到地址生成器中\n",
    "        yield detail_url # 返回一个地址生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc994748",
   "metadata": {},
   "source": [
    "7. 利用上面定义的函数 逐页提取所有电影的url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252770db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 15:13:14,488-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/1……\n",
      "2021-10-11 15:13:14,756-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/2……\n",
      "2021-10-11 15:13:15,024-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/3……\n",
      "2021-10-11 15:13:15,273-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/4……\n",
      "2021-10-11 15:13:15,556-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/5……\n",
      "2021-10-11 15:13:15,831-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/6……\n",
      "2021-10-11 15:13:16,108-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/7……\n",
      "2021-10-11 15:13:16,371-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/8……\n",
      "2021-10-11 15:13:16,638-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/9……\n",
      "2021-10-11 15:13:16,894-INFO:正在爬取https://static1.scrape.cuiqingcai.com/page/10……\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://static1.scrape.cuiqingcai.com/detail/1',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/2',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/3',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/4',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/5',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/6',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/7',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/8',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/9',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/10',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/11',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/12',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/13',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/14',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/15',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/16',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/17',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/18',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/19',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/20',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/21',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/22',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/23',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/24',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/36',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/25',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/26',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/27',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/28',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/29',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/30',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/31',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/32',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/33',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/34',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/35',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/37',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/38',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/39',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/40',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/41',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/42',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/43',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/44',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/45',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/46',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/47',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/48',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/49',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/50',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/51',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/52',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/53',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/54',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/55',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/56',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/57',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/58',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/59',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/60',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/61',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/62',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/63',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/64',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/65',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/66',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/67',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/68',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/69',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/70',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/71',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/72',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/73',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/74',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/75',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/76',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/77',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/78',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/79',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/80',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/81',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/82',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/83',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/84',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/85',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/86',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/87',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/88',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/89',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/90',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/91',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/92',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/93',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/94',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/95',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/96',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/97',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/98',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/99',\n",
       " 'https://static1.scrape.cuiqingcai.com/detail/100']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_url = []\n",
    "for page in range(1, TOTAL_PAGE+1):\n",
    "    html = scrape_index(page)\n",
    "    detail_url = parse_index(html)\n",
    "    for i in detail_url:\n",
    "        movie_url.append(i)\n",
    "movie_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238dfd5",
   "metadata": {},
   "source": [
    "8. 定义函数：复用scrape_page，做成一个新的scrape_detail方法。\n",
    "\n",
    "必要性说明：  \n",
    "你可能会问，这个scrape_detail方法里面只调用了scrape_page方法，没有别的功能，那爬取详情页直接用scrape_page方法不就好了，还有必要再单独定义scrape_detail方法吗？    \n",
    "答案是有必要，单独定义一个scrape_detail方法在逻辑上会显得更清晰，而且以后如果我们想要对scrape_detail方法进行改动，比如添加日志输出或是增加预处理，都可以在scrape_detail里面实现，而不用改动scrape_page方法，灵活性会更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293541b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_detail(url):\n",
    "    return scrape_page(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8c1fe",
   "metadata": {},
   "source": [
    "9. 定义函数：接收html.text代码, 返回电影详情页爬取结果的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8ba14e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-11 15:13:26,033-INFO:正在爬取https://static1.scrape.cuiqingcai.com/detail/1……\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cover': 'https://p0.meituan.net/movie/ce4da3e03e655b5b88ed31b5cd7896cf62472.jpg@464w_644h_1e_1c',\n",
       " 'name': '霸王别姬 - Farewell My Concubine',\n",
       " 'categories': ['剧情', '爱情'],\n",
       " 'published_at': '1993-07-26',\n",
       " 'drama': '影片借一出《霸王别姬》的京戏，牵扯出三个人之间一段随时代风云变幻的爱恨情仇。段小楼（张丰毅 饰）与程蝶衣（张国荣 饰）是一对打小一起长大的师兄弟，两人一个演生，一个饰旦，一向配合天衣无缝，尤其一出《霸王别姬》，更是誉满京城，为此，两人约定合演一辈子《霸王别姬》。但两人对戏剧与人生关系的理解有本质不同，段小楼深知戏非人生，程蝶衣则是人戏不分。段小楼在认为该成家立业之时迎娶了名妓菊仙（巩俐 饰），致使程蝶衣认定菊仙是可耻的第三者，使段小楼做了叛徒，自此，三人围绕一出《霸王别姬》生出的爱恨情仇战开始随着时代风云的变迁不断升级，终酿成悲剧。',\n",
       " 'score': 9.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_detail(html):\n",
    "    doc = pq(html)\n",
    "    cover = doc('img.cover').attr('src')\n",
    "    name = doc('a > h2').text()\n",
    "    categories = [item.text() for item in doc('.categories button span').items()]\n",
    "    published_at = doc('.info:contains( 上映 )').text()\n",
    "    published_at = re.search('(\\d{4}-\\d{2}-\\d{2})', published_at).group(1) \\\n",
    "        if published_at and re.search('\\d{4}-\\d{2}-\\d{2}', published_at) else None\n",
    "    drama = doc('.drama p').text()\n",
    "    score = doc('p.score').text()\n",
    "    score = float(score) if score else None\n",
    "    return {\n",
    "        'cover': cover,\n",
    "        'name': name,\n",
    "        'categories': categories,\n",
    "        'published_at': published_at,\n",
    "        'drama': drama,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "a = scrape_detail('https://static1.scrape.cuiqingcai.com/detail/1')\n",
    "a = parse_detail(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c4061",
   "metadata": {},
   "source": [
    "10. MongoDB数据库准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f90c66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb://localhost:27017/') # 连接MongoDB\n",
    "db = client['movies'] # 指定数据库\n",
    "collection = db['movies'] # 指定集合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ba181",
   "metadata": {},
   "source": [
    "11. 定义函数：将数据保存到MongoDB的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "549e97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data):\n",
    "    collection.update_one(\n",
    "        {'name': data.get('name')}, \n",
    "        {'$set': data}, \n",
    "        upsert=True # 存在即更新，不存在即插入\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6acf6",
   "metadata": {},
   "source": [
    "12. 抓取+保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f4133",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in movie_url:\n",
    "    html = scrape_detail(i)\n",
    "    data = parse_detail(html)\n",
    "    logging.info('爬取到数据：%s', data)\n",
    "    logging.info('保存MongoDB中')\n",
    "    save_data(data)\n",
    "    logging.info('数据保存成功')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7e6a8",
   "metadata": {},
   "source": [
    "### 多进程加速\n",
    "由于整个的爬取是单进程的，而且只能逐条爬取，速度稍微有点慢，有没有方法来对整个爬取过程进行加速呢？  \n",
    "在前面我们讲了多进程的基本原理和使用方法，下面我们就来实践一下多进程的爬取吧。  \n",
    "由于一共有 10  页详情页，并且这 10  页内容是互不干扰的，所以我们可以一页开一个进程来爬取。由于这 10  个列表页页码正好可以提前构造成一个列表，所以我们可以选用多进程里面的进程池 Pool  来实现这个过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def main(page):\n",
    "    index_html = scrape_index(page)\n",
    "    detail_urls = parse_index(index_html)\n",
    "    for detail_url in detail_urls:\n",
    "        detail_html = scrape_detail(detail_url)\n",
    "        data = parse_detail(detail_html)\n",
    "        logging.info('get detail data %s', data)\n",
    "        logging.info('saving data to mongodb')\n",
    "        save_data(data)\n",
    "        logging.info('data saved successfully')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool()\n",
    "    pages = range(1, TOTAL_PAGE + 1)\n",
    "    pool.map(main, pages)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71700232",
   "metadata": {},
   "source": [
    "这里我们首先给main方法添加一个参数page，用以表示列表页的页码。接着我们声明了一个进程池，并声明pages为所有需要遍历的页码，即1~10。最后调用map方法，第1个参数就是需要被调用的方法，第2个参数就是pages，即需要遍历的页码。  \n",
    "这样pages就会被依次遍历。把 1~10  这10个页码分别传递给main方法，并把每次的调用变成一个进程，加入到进程池中执行，进程池会根据当前运行环境来决定运行多少进程。比如我的机器的 CPU  有8个核，那么进程池的大小会默认设定为 8 ，这样就会同时有 8 个进程并行执行。\n",
    "运行输出结果和之前类似，但是可以明显看到加了多进程执行之后，爬取速度快了非常多。我们可以清空一下之前的MongoDB数据，可以发现数据依然可以被正常保存到MongoDB数据库中。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4f5d210",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
